{
    "collab_server" : "",
    "contents" : "---\ntitle: 'ST516: Computational Statistics Exam (Part A)'\nauthor: \"Katrine Eriksen and Katrine Bach\"\ndate: \"30 maj 2016\"\noutput: pdf_document\n---\n\n\n#Task 1\nIn this task we want to compare the effect of daily sport activity on the semester's grade avarage. To do this we use the data set sport.txt to calculate the following:\n\\begin{enumerate}\n\\item The correlation between the variables of interest.\n\\item The bootstrap estimate of correlation, \n\\item The bootstrap estimation of the standard error, \n\\item The Bias\n\\item The 95\\% considence interval. \n\\end{enumerate}\nTo begin with we will explain the theory behind the concepts that is needed. \n\n###Correlation:\n$E[X]$ is the expected value of the random variable $X$, this value is the weighted average of the possible values of $X$. The value of $E[X]$ does not say anything about the variantion of the these values. One way to measure this variance is to consider the average value of the squares of the difference between $X$ and $E[X]$. \n\n**Definition:** If $X$ is a random variable with mean $\\mu$, then the variance of $X$, denoted by $Var(X)$, is defined by \n$$Var(X)=E[(X-\\mu)^2]$$\n\nThe covariance between to random varibles can be defined by the following \n\n**Definition:** The covariance of two random variables $X$ and $Y$, denoted $Cov(X,Y)$, is defined by \n$$Cov(X,Y)=E[(X-\\mu_x)(Y-\\mu_y)]$$\nwhere $\\mu_x=E[X]$ and $\\mu_y=E[Y]$.\n\nThe correlation between two random variables $X$ and $Y$, denoted as $Cor(X,Y)$, is defined by \n$$Cor(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}$$\n\n\n###Boostrap:\nThe Boostrapping Technique was developed by Efron in 1979 and was inspired by the Jacknife method which is a method especially useful for variance and bias estimation. The boostrap technique are a nonparametric Monte Carlo method that estimate the population distribution by resampling from an observed sample. The resampling method allows us to estimate population charachteristics and make inference about them. We use this method when the populations mean $\\mu$ is unknown. \n\nSuppose that $X_1,\\dots,X_n$ are independent random variables that have a common distribution $F$. We are interested in using the variables to estimate some parameter $\\theta(F)$ which could be the mean of $F$. Suppose further that an estimator of $\\theta(F)$, where this estimator is called $g(X_1,\\dots,X_n)$, has been proposed. In order to judge its worth as an estimator of $\\theta(F)$ we are interested in estimationg its mean square error. \n$$MSE(F)\\equiv E_F[(g(X_1,\\dots X_n)-\\theta(F))^2]$$\n\nAs an immediate estimator of the above $MSE$ which is $S^2/n$ when $\\theta=E[X_i]$ and $g(X_1,\\dots,X_n)=\\bar{X}$, it is not that obvious how it can be estimated otherwise. We therefore present the boostrap technique for estimation the mean square error.\nWe first note that is the distribution function $F$ were known it would be possible to ompute the mean square error. Based on the observed data points $X_i$ it is possible to estimate the underlying distribution function $F$ by a so called empirical distribution function $F_e$\n$$F_e(x)=\\frac{\\text{number of i:}X_i\\leq x}{n}$$\n\nIf the empirical distribution function $F_e$ is close to $F$ which is should be in the case where $n$ is large. Then $\\theta(F_e)$ probably be close to $\\theta(F)$ when it is assumed that $\\theta$ is a continous function of the distribution. The $MSE(F)$ should approximately be equal to \n$$MSE(F_e)=E_{F_e}[(g(X_1,\\dots,X_n)-\\theta(F_e))^2]$$\n\nIn this expression the $X_i$ are to be regarded as being independent random variables having the distribution function $F_e$. This $MSE(F_e)$ is called the *Boostrap approximation to the mean square erroe $MSE(F)$*.\n\nFor the boostrap technique we can estimate the standard error by the following \n$$\\hat{se}=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{n}(\\hat{x_i}-\\bar{\\hat{x}})^2}$$\n\nTo make the bootstrap estimate of the bias, we use the following\n$$\\text{bias}(\\hat{\\theta})=E[\\hat{\\theta}-\\theta]=E[\\hat{\\theta}]-\\theta$$\nThe simplest form of the confidence interval relies on the central limit theorem. This implies that it requires a large sample to be effective. It is assumed that $\\hat{\\theta}$ is unbiased and we have a normal distribution. Then $\\theta$  is in the $Z$ interval \n$$\\hat{\\theta}\\pm \\Phi^{-1}\\bigg(1-\\frac{\\alpha}{2}\\bigg)$$\n$\\alpha$ is the p-value and because we want a 95% confidence interval, then $\\alpha$ should be $0.05$. \n\n\n\nNow we use the explained theory above to write a function, that answer the questions giving in the task. \n\n```{r}\nURL <- \"https://raw.githubusercontent.com/haghish/ST516/master/data/sport.txt\"\ndata <- read.table(URL, header = TRUE)\nx <- data$Sport\ny <- data$Grades\n\nbootstrap <- function(n,x,y){\n  #Warnings and stop\n  if (n<0){stop(\"n must be bigger than zero\")}\n\n  #Calculation of the correlation between the variables og interest.\n  theta.hat <- cor(x,y)\n\n  #Standard Error\n  set.seed(13)\n  N <- nrow(data) # sample size (number of rows)\n  storage <- numeric(n) #Store the variables\n  for (i in 1:n) {\n    k <- sample(1:N, size = N, replace = TRUE) # random indice\n    storage[i] <- cor(x[k],y[k])\n  }\n  se <- sd(storage) #standard error\n  hist(storage, probability = TRUE)\n\n  #bias of sample correlation\n  theta.hat.boot <- mean(storage)\n  bias <- theta.hat.boot - theta.hat\n\n  #95% confidence interval\n  alfa = 0.05\n  plus <- theta.hat.boot + qnorm(1-(alfa/2))*se\n  minus <- theta.hat.boot - qnorm(1-(alfa/2))*se\n  \n  #return the above culculations\n  return(list(\"exact_corr\"=theta.hat,\"boot_corr\"=theta.hat.boot, \"se\"=se, \"bias\"=bias, \"confidence_interval\"=c(plus, minus)))\n}\n\n\nbootstrap(10000, x, y)\n\n```\nBy using the R build-in function *cor* we found that the correlation between sport and grades are $cor(x,y)=0.6673$\n\nFrom our function we see that the bootstrap estimate of correlation is $0.6754$. It is possible to see that the two values of correlation lies close to each other, hence the bootstrap method for estimating the correlation is valid. It should not be expected that these values are exactly the same, as the bootstrap method only approximate the correlation.\n\nThe standard error of bootstrap estimation is found by using the formula from the theory. We get the standard error of the bootsrap estimation is $0.0698$.\nThis value can be interpreted as a measure of how good the approximation is. If the value is low this indicates that the approximation fits the observed values. On the other hand if the standard error is large the approximated values fits the observed values poorly. From this our standard error indicates that the bootstrap method approximate the values well, but there is still room for improvement. \n\nThe bias estimates the difference between the expected values of the approximation and the exact value. For a good approximation this implies that the diffence between the two values must be small. Our value is $0.0081$ which indicates that the estimator is close to being unbiased. \n\nA confidence interval of 95% reflects a significance level at 0.05. When it is stated that we are 95% confident that the true value of the parameter is in our confidence interval, we express that 95% of the hypothetically observed confidence intervals will hold the true value of the parameter. \nIn our case we found that the 95% confidence interval is $0.53852490-0.8122820$ which implicates that if the parameter lies within this interval we are 95% confident that the paramter estimate is true, hence it can be accepted. Our bootstrap estimate is $0.0698$ which is in the confidence interval, hence the parameter can be accepted. \n\n\n***\n\n#Task 2\nIn this task we have to simulate two problems \"Buffon's Needle Problem\" and \"Lazzarini's experiment\". First we want to explain the idea of \"Buffon's Needle Problem\" and cover the theory we use. \n\n###Buffon's Needle Problem: \nBuffon's needle problem is a question that was first posed in the 18 th century by Georges-Luis Leclerc. The problem was the following. \nSuppose we have an infinit floor with parallel lines and we let $d$ be the distance between the lines on the floor. If we then drop a needle onto the floor, what is the probability that the needle will lie across a line?\nFor solving this problem let the needle have a length $l$ with $y$ being the distance from the lower end of the needle to the nearest gridline above it. Further more $\\theta$ measures the smallest clockwise angle form the gridirection to the needle. \nIn the case where $l\\leq d$, then the probability that the needle is crossing a line on the floor is\n$$P(hit)=\\frac{\\int^{\\pi}_0 l\\sin\\theta d\\theta}{\\pi d}=\\frac{2l}{\\pi d}\\approx 0.6366$$\nThe needle hits one of the gridlines if and only iff $y<l\\sin\\theta$\n\n\n###Monte Carlo: \nSuppose we want to compute $\\theta$ where \n$$\\theta=\\int_0^1 g(x)dx$$\nTo compute the value of $\\theta$ then we can express $\\theta$ as \n$$\\theta= E[g(U)]$$\nwhere U is uniformly distributed over (0,1) for which it follows that $g(U_1),\\dots,g(U_n)$ are independant and identically distributed with the mean $\\theta$. By the strong law of large numbers it follows that \n$$\\sum_{i=1}^n \\frac{g(U_i)}{n}\\rightarrow E[g(U)]=\\theta \\text{ as } n\\rightarrow \\infty$$\nHence it is possible to approximate $\\theta$ by generating a large number of random numbers $u_i$ and taking as our approximation the avarge value of $g(u_i)$. This approach is called *the Monte Carlo* method. \n\nIn the case where we want to compute\n$$\\theta=\\int_a^b g(x)dx$$\nthen by the use of substitution $y=\\frac{x-a}{b-a}$, $dy=\\frac{dx}{b-a}$ form which we can see that \n\\begin{align*}\n\\theta &= \\int_0^1 g(a+(b-a)y)(b-a)dy\\\\\n&= \\int_0^1 h(y)dy\n\\end{align*}\n\nthus we can approximate $\\theta$ by continually generating random numbers and then taking the average value of *h* evaluated at these random numbers.\n\nIn our case we can, by the use of the *Monte Carlo* method, write $$\\int^{\\pi}_0 l\\sin\\theta d\\theta$$ \nas \n$$\\frac{\\sum_{i=1}^n f((b-a)*U_i+a)}{n}(b-a)$$ \n\n\n###Standard Deviation:\nThe standard deviation expresses how a stochastic variable is located around its mean value.\n$$\\sigma=\\sqrt{E[(X-E[X])^2]}$$\n\n\n###Standard Error:\nThe standard error is calculated by the use of the standard deviation $\\sigma$. It is usually estimated by the standard deviation divided bu the square root of the sample size.\n$$\\text{se} = \\frac{\\sigma}{\\sqrt(n)}$$\n\n\n###Lazzarini's experiment: \nIn 1901 the italian mathematician Mario Lazzarini perfomed \"Buffon's Needle experiment\". He tossed a needle $3408$ times and obtained the well-known estimate $\\frac{355}{133}$ for $\\pi$ which was accurate to the six significant digits. Lazzarini chose needles of a length $5/6$ of the width of the stripes. In this case he found that the probability that the needles will cross the line is $\\frac{5}{3\\pi}$\n\n\nIn the first subtask we have to estimate the \"Buffon's Needle experiment\" using the Monte Carlo method and report the **P(hit)**. In addtion we have to compute and report the variance, standard deviation, standard error and the 95% confidence interval. This have we done in the following program.\n\n```{r}\n#opgave 1\nintegralet <- 0\nN <- 10000\nl <- 1\nd <- 1\n# estimating the integration\nfor (i in 1:N) {\n  a <- 0\n  b <- pi\n  U <- runif(N)\n  theta <- (b-a)*U+a\n  f <- l*sin(theta)\n  integralet[i] <- sum((f/N))*(b-a) #the estiamtion\n  Phit<- integralet[i]/(pi*d)\n  pie <- (2*l)/(Phit*d)\n}\n#Variance\nvarians <- var(integralet)\nprint(paste(\"Variance:\", varians))\n\n#Standard Deviation\ndeviation <- sd(integralet)\nprint(paste(\"Standard deviation:\", deviation))\n\n#Standard Error\nerror <- deviation/(sqrt(N))\nprint(paste(\"Standard Error:\", error))\n\n#the 95% confidence interval\nalfa = 0.05\nplus <- integralet + qnorm(1-(alfa/2))*error\nminus <- integralet - qnorm(1-(alfa/2))*error\nprint(c(mean(minus),mean(plus)))\n\n\n\n```\n\nIn subtask 2 we should estimate **P(hit)** by performing \"Buffon's Needle experiment\". To do this we write a function that takes 3 arguments (N,l,d) and return the estimated value $\\pi$. We have been given that $N=10000$ and $l=d=1$\n\n```{r}\n#Opgave 2\nbuffon <- function(N,l,d){\n  a <- 0\n  b <- pi\n  U <- runif(N)\n  theta <- (b-a)*U+a\n  f <- l*sin(theta)\n  integralet <- sum((f/N))*(b-a) #the estiamtion\n  Phit<- integralet/(pi*d)\n  pie <- (2*l)/(Phit*d)\n  return(pie)\n}\nbuffon(10000,1,1)\n```\n\nHere we see that the estimated pi we get is pretty close to the actuel pi which is $3.14159265$. Obviously our estimate could be better, but we think it is an acceptable estimating of $\\pi$.\n\nIn subtask 3 we have to repeat subtask 2 for values ranging from 1 to 10000 in intervals of 10. To do this we make a for-loop so we can se what the estimation for $pi$ is in every 10th interval. \n\n```{r}\n#Opgave 3\nestpi <- 0\nfor (i in seq(1, 10000, 10)) {\n  estpi[i]<-buffon(i,1,1)\n  #print(Task2(i,1,1))\n}\nplot(estpi)\n\n```\n\nFrom this plot we see that when N goes to infity then our estimation of $\\pi$ goes towards the exact $pi$\n\nThe last subtask is about Lazzarini's famous estimation of $pi$ using \"Buffon's Needle experiment\", which is accurate to the 6th decimal. We have to replicate his experiment a 10000 times where $N=3408$, $l=2.5$ and $d=3$, and compare them to our own results. \n\n```{r}\n#Opgave 4\nlazzarini <- 0\nfor(i in seq(1,10000,1)){\n  lazzarini[i] <- buffon(3408,2.5,3)\n}\nplot(lazzarini)\n\n```\n\n*Skal omskrives*\nFrom the plot we se that the Lazzarini estimation of $\\pi$ is centered in the interval $3.10-3.17$. We get a plot that is more detailed by the Lazzarini estimation of $\\pi$ than our own, this implies that the Lazzarini experiment is better than our estimation of $\\pi$. In our model we have estimations that varies from $2.7-4.4$ but most of them is centered in the interval $3.0-3.3$ which is not as good at the Lazzarini's experiment. We can therefore conclude that because Lazzarini have chosen $N=3408$, $l=2.5$ and $d=3$, his estimation of $\\pi$ is much better than our estimation. \n\nWe think Lazzarini's choice of N is a little inapproriate because of the fact that it seems to specific. It would be more reliable if there was some notes about the devitation of this number, because seems as if Lazzerina only chose this value because he aldready knew that he should get something like $\\frac{355}{113}$ for running the eksperiment multiple times. \n\n\n\n\n\n***\n\n#Task 3\nA company creates gum for children. These gums comes with a photo of a soccerplayer. Gitte has purchased 301 cards.\n\n\nThis task is about whether we reject or accept a p-value. Again we first start by explaining the teory we use, which is $chi^2$-distribution and Goodness of fit. \n\n#####$\\chi^2$-distribution:\nIn the case where $Y_i$  have normal independent distribution with mean $0$ and variance $1$ then $\\chi^2$ distribution can be defined as \n$$\\chi^2=\\sum_{i=1}^{df} Y_i^2$$ \nwhere $df$ is the *degree of freedom*. The *degree og freedom* is the number of parameters which may be independently varied minus $1$\n\n#####Goodness of fit: \nThe idea of the Pearson chi-square test is to compare the observed sample points with the expected sample points. The hypothesis $H_0$ is rejected if the observed and expected numbers are too different.\n\nWe let $N_j$ denote the observed number of sample points in $A_j$\n$$N_j=\\sum_{i=0}^n I_{A_j}(X_i)$$\nThe expected number under $H_0$ is given by:\n$$E_{H_0}=\\sum_{i=1}^n E_{H_0}{I_{A_j}(X_i)}=nP_{H_0}(X_i\\in A_j)$$\nThe Pearson chi-square statistic combine $N_j$ and $E_{H_0}$ into a single expression in the following way \n$$\\chi^2=\\sum_{j=1}^k \\frac{(N_j-E_{H_0}(N_j))^2}{E_{H_0}(N_j)}$$\nWe reject $H_0$ when the observed value of $\\chi^2$ is too large.\n\nThe p-value gives the probability of obtaining an outcome that is more extreme than the observed one if $H_0$ is true\n$$p-value=P(\\chi^2_{k-r-1}>\\chi^2_{obs})$$\nwhere $\\chi^2_{k-r-1}$ is denoting a chi-square random variables with $k-r-1$ degrees of freedom. We reject $H_0$ if the p-value$<\\alpha$.\n\nIn the first subtask we have to write a function that applies the Monte Carlo method and generate a $\\chi^2$ distribution. The function we make should return the probability that the $\\chi^2$ distribution is larger than the value x. \n\n```{r}\nchi.probability <-function(x, df, n){\nset.seed(13)\nsumm <- 0\nY <- 0\n  for (i in 1:df) {\n    Y <- rnorm(n, mean=0, sd=1) \n    Yianden <- Y^2\n    summ <- summ + Yianden\n  }\nblup <- ecdf(summ)\nplot(blup)\np <-1-blup(x)\nreturn(p)\n\n}\nchi.probability(10,4,100)\n\n```\n\nIn the second subtask we have to write a function that takes two variables (x,p). This function should return the $\\chi^2$ goodness of fit value.\n```{r}\nGoodnessOfFit <- function(x,p=c(rep(1/length(x),length(x)))){\n  set.seed(13)\n  n<-301\n  Exp<-n*p\n  Pearson<-((x-Exp)^2/Exp)\n  GOF<-sum(Pearson,na.rm = FALSE)\n  return(GOF)\n}\n\n```\n\nIn this function we have made the precautions that the amount of observable values cannot be equal to zero, while this would imply that the probabilities within the p-vector was unfined. \nThe length of the vector containing the observed values cannot be zero, since it is not possible to have a negative amount of observations. This also counts for the p-vector. \nOne of the probabilities within the p-vector can be zero because then the *Pearson* step will be unfinied. \n\nFor the fourth subtask the company aclaims that all cards are equally likely to get in any shop. For validating this we make the following $H_0$ together with $H_1$ :\n\n$H_0$: It is equally likely to get each of the different cards, this implies that the probability for getting each card is $1/20$. Because the company claims that all cards are equally likely to get in any shop, since there a 20 different cards this implicates that the probability for getting each card is $1/20$\n\n$H_1$: It is not equally likely to get each of the different cards. This is just the opposite of our nul-hypothesis, so the H1-hypothesis implies that the probability for getting each card is not $1/20$.\n\n```{r}\nURL <- \"https://raw.githubusercontent.com/haghish/ST516/master/data/soccer.txt\"\ndata <- read.table(URL, header = TRUE)\nx<-data$Number\nGOFit<-GoodnessOfFit(x)\nchi.probability(GOFit,length(x)-1,1000)\n```\nIt can be seen that the p-value is zero. By interpreting this value the null-hypothesis can be rejected. Hence all cards are not equally distributed. This is also consistent with the observed values from Gitte. \n\nFor subtask five we want to show the difference between the $\\chi^2$ provided by *R* in contrast to the function for $\\chi^2$ by Monte Carlo made in one of the previous subtasks. \n```{r}\n1-pchisq(GOFit,length(x)-1)\n\n```\nIt can be seen that these two p-values are not the same. Eventhough these values are not the same, they both reach the same conclusion that the $H_0$ hypothesis must be rejected. The difference is that the build-in function within *R* calculates the intregral numerically, while our function only makes an approximation of the integral using the Monte Carlo method.\n\n\nThe $P(\\chi^2>x$ can be used to obtain the p-value, because this calculates the upper tail of the distribution. \n\nSince the model is a chi-squared Goodness of fit test the chi-square distribution is used to evaluate the observed data. The chi-squared distribution is used to find the lower tail, which is the sum of probability until x. The probability that $\\chi^2>x$ is then 1 minus the lower tail. \n\nThe p-value indicates how good the observed data fits the distribution. If the p-value is low this indicate that the obsereved data is unlikeli according to the null-hypothesis, which implies that the null-hypothesis must be wrong, hence it is rejected. \n\n\n\nIn subtask 7 we assume that Gittes friend collects a large number of cards and calculates the probability of obtaining each player's card. We will make use of our functions *chi.probability* and *GoodnessOfFit* to check if Gitte's cards fit the expected distribution. This implies that the null-hypothesis in this case is that Gitte's cards fits the expected distribution calculated by her friend. \n\n```{r}\nURL <- \"https://raw.githubusercontent.com/haghish/ST516/master/data/soccer.txt\"\ndata <- read.table(URL, header = TRUE)\nx<-data$Number\np<-data$expected\nGOFit<-GoodnessOfFit(x,p)\nchi.probability(GOFit,length(x)-1,1000)\n\n```\nBy looking at the output it can be seen that the expected distribution calculated by Gitte's friend fits the distribution of Gitte's cards better than the claim from the company. It can be seen from the p-value in this case that the null-hypothesis is not rejected. \n\n\n***\n\n\n\n\n",
    "created" : 1465045440289.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2299426246",
    "id" : "EA05C04D",
    "lastKnownWriteTime" : 1465219052,
    "last_content_update" : 1465219052054,
    "path" : "~/Google Drev/ST505/ST516/ExamST516/R/blupblup.Rmd",
    "project_path" : "R/blupblup.Rmd",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}